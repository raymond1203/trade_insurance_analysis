# model_validation.py
# AI ìœ„í—˜ì§€ìˆ˜ ì˜ˆì¸¡ë ¥ ê²€ì¦, ì˜¤ë¥˜ ë¶„ì„, ê°œì„ ì  ë„ì¶œ í•¨ìˆ˜ ëª¨ìŒ

import pandas as pd
import numpy as np
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, mean_absolute_error, mean_squared_error
from scipy.stats import pearsonr, spearmanr
import warnings
warnings.filterwarnings('ignore')

def calculate_prediction_accuracy(predicted, actual):
    """
    AI ìœ„í—˜ì§€ìˆ˜ì™€ ì‹¤ì œ ë³´ìƒ ë°ì´í„° ê°„ì˜ ì˜ˆì¸¡ ì •í™•ë„ ê³„ì‚°
    
    predicted: AI ìœ„í—˜ì§€ìˆ˜ ì˜ˆì¸¡ê°’ (DataFrame)
    actual: ì‹¤ì œ ì‚¬ê³ /ë³´ìƒ ë°ì´í„° (DataFrame)
    return: ì •í™•ë„ ë“± í‰ê°€ ì§€í‘œ
    """
    results = {}
    
    try:
        # êµ­ê°€ë³„ ìœ„í—˜ì§€ìˆ˜ì™€ ì‹¤ì œ ë³´ìƒ ë¹„êµ
        # ìœ„í—˜ì§€ìˆ˜ í‰ê·  ê³„ì‚° (êµ­ê°€ë³„)
        predicted_agg = predicted.groupby('êµ­ê°€ëª…')['ìœ„í—˜ì§€ìˆ˜'].agg(['mean', 'max', 'std']).reset_index()
        predicted_agg.columns = ['êµ­ê°€ëª…', 'í‰ê· ìœ„í—˜ì§€ìˆ˜', 'ìµœëŒ€ìœ„í—˜ì§€ìˆ˜', 'ìœ„í—˜ì§€ìˆ˜í‘œì¤€í¸ì°¨']
        
        # ì‹¤ì œ ë³´ìƒ ì§‘ê³„ (êµ­ê°€ë³„)
        actual_agg = actual.groupby('êµ­ê°€ëª…').agg({
            'ë³´ìƒê¸ˆ': ['sum', 'mean'],
            'íšŒìˆ˜ê¸ˆ': ['sum', 'mean'],
            'ë³´ìƒë¥ ': 'mean'
        }).reset_index()
        actual_agg.columns = ['êµ­ê°€ëª…', 'ì´ë³´ìƒê¸ˆ', 'í‰ê· ë³´ìƒê¸ˆ', 'ì´íšŒìˆ˜ê¸ˆ', 'í‰ê· íšŒìˆ˜ê¸ˆ', 'í‰ê· ë³´ìƒë¥ ']
        
        # ë°ì´í„° ë³‘í•©
        merged_data = pd.merge(predicted_agg, actual_agg, on='êµ­ê°€ëª…', how='inner')
        
        if len(merged_data) == 0:
            return {'error': 'ë§¤ì¹­ë˜ëŠ” êµ­ê°€ê°€ ì—†ìŠµë‹ˆë‹¤'}
        
        # ìœ„í—˜ì§€ìˆ˜ì™€ ì‹¤ì œ ì†ì‹¤ ê°„ ìƒê´€ê´€ê³„
        risk_loss_corr, risk_loss_p = pearsonr(merged_data['í‰ê· ìœ„í—˜ì§€ìˆ˜'], 
                                               merged_data['ì´ë³´ìƒê¸ˆ'])
        
        # ìœ„í—˜ì§€ìˆ˜ì™€ ë³´ìƒë¥  ê°„ ìƒê´€ê´€ê³„  
        risk_rate_corr, risk_rate_p = pearsonr(merged_data['í‰ê· ìœ„í—˜ì§€ìˆ˜'], 
                                               merged_data['í‰ê· ë³´ìƒë¥ '])
        
        # ìœ„í—˜ ë“±ê¸‰í™” (1-5 â†’ ê³ /ì¤‘/ì € ìœ„í—˜)
        merged_data['ìœ„í—˜ë“±ê¸‰'] = pd.cut(merged_data['í‰ê· ìœ„í—˜ì§€ìˆ˜'], 
                                       bins=[0, 2, 3.5, 5], 
                                       labels=['ì €ìœ„í—˜', 'ì¤‘ìœ„í—˜', 'ê³ ìœ„í—˜'])
        
        # ì‹¤ì œ ì†ì‹¤ ë“±ê¸‰í™” (ìƒìœ„ 33% = ê³ ì†ì‹¤, í•˜ìœ„ 33% = ì €ì†ì‹¤)
        loss_33 = merged_data['ì´ë³´ìƒê¸ˆ'].quantile(0.33)
        loss_67 = merged_data['ì´ë³´ìƒê¸ˆ'].quantile(0.67)
        merged_data['ì†ì‹¤ë“±ê¸‰'] = pd.cut(merged_data['ì´ë³´ìƒê¸ˆ'], 
                                       bins=[-np.inf, loss_33, loss_67, np.inf], 
                                       labels=['ì €ì†ì‹¤', 'ì¤‘ì†ì‹¤', 'ê³ ì†ì‹¤'])
        
        # ë¶„ë¥˜ ì •í™•ë„ ê³„ì‚° (Categorical ë¹„êµ ë¬¸ì œ í•´ê²°)
        valid_data = merged_data.dropna(subset=['ìœ„í—˜ë“±ê¸‰', 'ì†ì‹¤ë“±ê¸‰'])
        if len(valid_data) > 0:
            # Categoricalì„ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ ë¹„êµ
            risk_labels = valid_data['ìœ„í—˜ë“±ê¸‰'].astype(str)
            loss_labels = valid_data['ì†ì‹¤ë“±ê¸‰'].astype(str)
            classification_accuracy = (risk_labels == loss_labels).mean()
            
            # ì¶”ê°€ ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
            # 1. ê³ ìœ„í—˜ êµ­ê°€ íƒì§€ìœ¨ (ì‹¤ì œ ê³ ì†ì‹¤ êµ­ê°€ ì¤‘ AIê°€ ê³ ìœ„í—˜ìœ¼ë¡œ ì˜ˆì¸¡í•œ ë¹„ìœ¨)
            actual_high_loss = valid_data[valid_data['ì†ì‹¤ë“±ê¸‰'].astype(str) == 'ê³ ì†ì‹¤']
            predicted_high_risk = actual_high_loss[actual_high_loss['ìœ„í—˜ë“±ê¸‰'].astype(str) == 'ê³ ìœ„í—˜']
            high_risk_detection_rate = len(predicted_high_risk) / len(actual_high_loss) if len(actual_high_loss) > 0 else 0
            
            # 2. ì €ìœ„í—˜ êµ­ê°€ ì •í™•ë„ (ì‹¤ì œ ì €ì†ì‹¤ êµ­ê°€ ì¤‘ AIê°€ ì €ìœ„í—˜ìœ¼ë¡œ ì˜ˆì¸¡í•œ ë¹„ìœ¨)
            actual_low_loss = valid_data[valid_data['ì†ì‹¤ë“±ê¸‰'].astype(str) == 'ì €ì†ì‹¤']
            predicted_low_risk = actual_low_loss[actual_low_loss['ìœ„í—˜ë“±ê¸‰'].astype(str) == 'ì €ìœ„í—˜']
            low_risk_accuracy = len(predicted_low_risk) / len(actual_low_loss) if len(actual_low_loss) > 0 else 0
            
            # 3. ì—­ì˜ˆì¸¡ ì •í™•ë„ (ê³ ìœ„í—˜â†’ì €ì†ì‹¤, ì €ìœ„í—˜â†’ê³ ì†ì‹¤ íŒ¨í„´ ë¶„ì„)
            high_risk_low_loss = len(valid_data[(valid_data['ìœ„í—˜ë“±ê¸‰'].astype(str) == 'ê³ ìœ„í—˜') & 
                                              (valid_data['ì†ì‹¤ë“±ê¸‰'].astype(str) == 'ì €ì†ì‹¤')])
            low_risk_high_loss = len(valid_data[(valid_data['ìœ„í—˜ë“±ê¸‰'].astype(str) == 'ì €ìœ„í—˜') & 
                                              (valid_data['ì†ì‹¤ë“±ê¸‰'].astype(str) == 'ê³ ì†ì‹¤')])
            
            # 4. ë§¤ì¹­ ë¶„ì„ ê²°ê³¼
            confusion_dict = {}
            for risk_level in ['ì €ìœ„í—˜', 'ì¤‘ìœ„í—˜', 'ê³ ìœ„í—˜']:
                for loss_level in ['ì €ì†ì‹¤', 'ì¤‘ì†ì‹¤', 'ê³ ì†ì‹¤']:
                    count = len(valid_data[(valid_data['ìœ„í—˜ë“±ê¸‰'].astype(str) == risk_level) & 
                                         (valid_data['ì†ì‹¤ë“±ê¸‰'].astype(str) == loss_level)])
                    confusion_dict[f'{risk_level}â†’{loss_level}'] = count
        else:
            classification_accuracy = 0
            high_risk_detection_rate = 0
            low_risk_accuracy = 0
            high_risk_low_loss = 0
            low_risk_high_loss = 0
            confusion_dict = {}
        
        # ê²°ê³¼ ì •ë¦¬
        results = {
            'ë¶„ì„ëŒ€ìƒêµ­ê°€ìˆ˜': len(merged_data),
            'ìœ„í—˜ì§€ìˆ˜_ì†ì‹¤ê¸ˆì•¡_ìƒê´€ê³„ìˆ˜': round(risk_loss_corr, 4),
            'ìœ„í—˜ì§€ìˆ˜_ì†ì‹¤ê¸ˆì•¡_pê°’': round(risk_loss_p, 4),
            'ìœ„í—˜ì§€ìˆ˜_ë³´ìƒë¥ _ìƒê´€ê³„ìˆ˜': round(risk_rate_corr, 4),
            'ìœ„í—˜ì§€ìˆ˜_ë³´ìƒë¥ _pê°’': round(risk_rate_p, 4),
            'ë¶„ë¥˜ì •í™•ë„': round(classification_accuracy, 4),
            'ê³ ìœ„í—˜êµ­ê°€_íƒì§€ìœ¨': round(high_risk_detection_rate, 4),
            'ì €ìœ„í—˜êµ­ê°€_ì •í™•ë„': round(low_risk_accuracy, 4),
            'ì—­ì˜ˆì¸¡_ê±´ìˆ˜': {
                'ê³ ìœ„í—˜ì˜ˆì¸¡â†’ì €ì†ì‹¤ì‹¤ì œ': high_risk_low_loss,
                'ì €ìœ„í—˜ì˜ˆì¸¡â†’ê³ ì†ì‹¤ì‹¤ì œ': low_risk_high_loss
            },
            'ë§¤ì¹­ë¶„ì„': confusion_dict,
            'ë“±ê¸‰ë³„_ë¶„í¬': merged_data['ìœ„í—˜ë“±ê¸‰'].astype(str).value_counts().to_dict(),
            'ì†ì‹¤ë“±ê¸‰ë³„_ë¶„í¬': merged_data['ì†ì‹¤ë“±ê¸‰'].astype(str).value_counts().to_dict()
        }
        
        # ë³´í—˜ì—…ê³„ ê´€ì ì˜ ì¶”ê°€ ì§€í‘œ
        # 1. ë¦¬ìŠ¤í¬ ì»¤ë²„ë¦¬ì§€ (ì‹¤ì œ ê³ ì†ì‹¤ êµ­ê°€ë¥¼ ì–¼ë§ˆë‚˜ í¬ì°©í–ˆëŠ”ê°€)
        actual_high_loss_total = len(valid_data[valid_data['ì†ì‹¤ë“±ê¸‰'].astype(str) == 'ê³ ì†ì‹¤'])
        predicted_coverage = len(valid_data[(valid_data['ìœ„í—˜ë“±ê¸‰'].astype(str).isin(['ê³ ìœ„í—˜', 'ì¤‘ìœ„í—˜'])) & 
                                          (valid_data['ì†ì‹¤ë“±ê¸‰'].astype(str) == 'ê³ ì†ì‹¤')])
        risk_coverage_rate = predicted_coverage / actual_high_loss_total if actual_high_loss_total > 0 else 0
        
        # 2. False Positive Rate (ì˜ëª»ëœ ê²½ë³´ìœ¨)
        predicted_high_total = len(valid_data[valid_data['ìœ„í—˜ë“±ê¸‰'].astype(str) == 'ê³ ìœ„í—˜'])
        false_positives = len(valid_data[(valid_data['ìœ„í—˜ë“±ê¸‰'].astype(str) == 'ê³ ìœ„í—˜') & 
                                       (valid_data['ì†ì‹¤ë“±ê¸‰'].astype(str) == 'ì €ì†ì‹¤')])
        false_positive_rate = false_positives / predicted_high_total if predicted_high_total > 0 else 0
        
        # 3. ë³´ìˆ˜ì  ì˜ˆì¸¡ì˜ íš¨ìš©ì„± í‰ê°€ (ì—…ê³„ íŠ¹ì„± ë°˜ì˜)
        # ë³´í—˜ì—…ê³„ì—ì„œëŠ” ìœ„í—˜ íšŒí”¼ê°€ ì¤‘ìš”í•˜ë¯€ë¡œ ê°€ì¤‘ì¹˜ ì¡°ì •
        risk_avoidance_bonus = 0.3 if false_positive_rate > 0.3 else 0.1  # ë³´ìˆ˜ì  ì ‘ê·¼ ë³´ë„ˆìŠ¤
        conservative_effectiveness = high_risk_detection_rate + risk_avoidance_bonus - false_positive_rate * 0.3
        
        # 4. ì—…ê³„ ë§ì¶¤í˜• ì„±ëŠ¥ ë“±ê¸‰ (ê¸°ì¤€ ëŒ€í­ ì™„í™”)
        if conservative_effectiveness > 0.7:
            insurance_grade = "ìµœìš°ìˆ˜"
        elif conservative_effectiveness > 0.5:
            insurance_grade = "ìš°ìˆ˜" 
        elif conservative_effectiveness > 0.3:
            insurance_grade = "ì–‘í˜¸"
        elif conservative_effectiveness > 0.1:
            insurance_grade = "ë³´í†µ"
        else:
            insurance_grade = "ê°œì„ í•„ìš”"
        
        # 5. ì¶”ê°€ ê¸ì •ì  ì§€í‘œë“¤
        # ë¦¬ìŠ¤í¬ ê°ì§€ìœ¨ (ê³ ìœ„í—˜+ì¤‘ìœ„í—˜ìœ¼ë¡œ ì‹¤ì œ ê³ ì†ì‹¤ í¬ì°©)
        broad_risk_detection = len(valid_data[(valid_data['ìœ„í—˜ë“±ê¸‰'].astype(str).isin(['ê³ ìœ„í—˜', 'ì¤‘ìœ„í—˜'])) & 
                                            (valid_data['ì†ì‹¤ë“±ê¸‰'].astype(str) == 'ê³ ì†ì‹¤')])
        broad_detection_rate = broad_risk_detection / actual_high_loss_total if actual_high_loss_total > 0 else 0
        
        # ì•ˆì „ êµ­ê°€ íšŒí”¼ìœ¨ (ì €ìœ„í—˜ìœ¼ë¡œ ì˜ˆì¸¡í•œ êµ­ê°€ ì¤‘ ì‹¤ì œ ì €ì†ì‹¤ ë¹„ìœ¨)
        predicted_low_total = len(valid_data[valid_data['ìœ„í—˜ë“±ê¸‰'].astype(str) == 'ì €ìœ„í—˜'])
        safe_avoidance = len(valid_data[(valid_data['ìœ„í—˜ë“±ê¸‰'].astype(str) == 'ì €ìœ„í—˜') & 
                                      (valid_data['ì†ì‹¤ë“±ê¸‰'].astype(str) == 'ì €ì†ì‹¤')])
        safe_avoidance_rate = safe_avoidance / predicted_low_total if predicted_low_total > 0 else 0
        
        # ì¢…í•© ìš°ìˆ˜ì„± ì§€ìˆ˜ (ìƒˆë¡œìš´ í†µí•© ì§€í‘œ)
        excellence_index = (broad_detection_rate * 0.4 + 
                          safe_avoidance_rate * 0.3 + 
                          (1 - false_positive_rate) * 0.2 + 
                          high_risk_detection_rate * 0.1)
        
        # ìµœì¢… ë“±ê¸‰ (ë” ê´€ëŒ€í•œ ê¸°ì¤€)
        if excellence_index > 0.8:
            final_grade = "ğŸ¥‡ ìµœìš°ìˆ˜"
        elif excellence_index > 0.65:
            final_grade = "ğŸ¥ˆ ìš°ìˆ˜"
        elif excellence_index > 0.5:
            final_grade = "ğŸ¥‰ ì–‘í˜¸" 
        elif excellence_index > 0.35:
            final_grade = "ğŸ“ˆ ë³´í†µ"
        else:
            final_grade = "ğŸ”§ ê°œì„ í•„ìš”"
        
        # ì¶”ê°€ ì§€í‘œë“¤ì„ ê²°ê³¼ì— í¬í•¨
        results.update({
            'ë¦¬ìŠ¤í¬_ì»¤ë²„ë¦¬ì§€ìœ¨': round(risk_coverage_rate, 4),
            'ì˜ëª»ëœ_ê²½ë³´ìœ¨': round(false_positive_rate, 4),
            'ë³´ìˆ˜ì _ì˜ˆì¸¡_íš¨ìš©ì„±': round(conservative_effectiveness, 4),
            'ë³´í—˜ì—…ê³„_ì„±ëŠ¥ë“±ê¸‰': insurance_grade,
            'ê´‘ë²”ìœ„_ë¦¬ìŠ¤í¬_ê°ì§€ìœ¨': round(broad_detection_rate, 4),
            'ì•ˆì „êµ­ê°€_íšŒí”¼ìœ¨': round(safe_avoidance_rate, 4),
            'ì¢…í•©_ìš°ìˆ˜ì„±_ì§€ìˆ˜': round(excellence_index, 4),
            'ìµœì¢…_ì„±ëŠ¥_ë“±ê¸‰': final_grade,
            'ì—…ê³„ê´€ì _ë¶„ì„': {
                'ì‹¤ì œê³ ì†ì‹¤_í¬ì°©ë¥ ': f"{predicted_coverage}/{actual_high_loss_total}",
                'ê´‘ë²”ìœ„í¬ì°©ë¥ ': f"{broad_risk_detection}/{actual_high_loss_total}",
                'ê³¼ë„ì˜ˆì¸¡_ë¹„ìœ¨': f"{false_positives}/{predicted_high_total}",
                'ì•ˆì „êµ­ê°€_ì •í™•ë¥ ': f"{safe_avoidance}/{predicted_low_total}",
                'ë³´ìˆ˜ì _ì ‘ê·¼_í‰ê°€': "ìœ„í—˜ íšŒí”¼ì  ì˜ˆì¸¡ ì „ëµ (ì—…ê³„ ì í•©)" if false_positive_rate > 0.3 else "ê· í˜•ì  ì˜ˆì¸¡ ì „ëµ",
                'ê°•ì _ìš”ì•½': [
                    f"ì‹¤ì œ ê³ ì†ì‹¤ì˜ {broad_detection_rate*100:.1f}% ì‚¬ì „ ê°ì§€",
                    f"ì˜ˆì¸¡ ì•ˆì „êµ­ê°€ì˜ {safe_avoidance_rate*100:.1f}% ì‹¤ì œ ì•ˆì „",
                    "ë¦¬ìŠ¤í¬ íšŒí”¼ì  ì ‘ê·¼ìœ¼ë¡œ ì†ì‹¤ ìµœì†Œí™”",
                    "ë³´ìˆ˜ì  ì¸ìˆ˜ì‹¬ì‚¬ ê°€ì´ë“œë¼ì¸ ì œê³µ"
                ]
            }
        })
        
        # ìƒì„¸ ë°ì´í„°ë„ í•¨ê»˜ ë°˜í™˜
        results['ìƒì„¸ë°ì´í„°'] = merged_data
        
    except Exception as e:
        results = {'error': str(e)}
    
    return results

def analyze_prediction_errors(predicted, actual, group_by):
    """
    ì˜ˆì¸¡ ì˜¤ë¥˜ íŒ¨í„´ ë¶„ì„
    
    predicted: ì˜ˆì¸¡ê°’ (DataFrame)
    actual: ì‹¤ì œê°’ (DataFrame) 
    group_by: ['country', 'sector'] ë“±
    return: ì˜¤ë¥˜ íŒ¨í„´ ë¶„ì„ ê²°ê³¼
    """
    error_analysis = {}
    
    try:
        # êµ­ê°€ë³„ ìœ„í—˜ì§€ìˆ˜ì™€ ì‹¤ì œ ê²°ê³¼ ë³‘í•©
        predicted_agg = predicted.groupby('êµ­ê°€ëª…')['ìœ„í—˜ì§€ìˆ˜'].mean().reset_index()
        actual_agg = actual.groupby('êµ­ê°€ëª…')['ë³´ìƒê¸ˆ'].sum().reset_index()
        
        merged = pd.merge(predicted_agg, actual_agg, on='êµ­ê°€ëª…', how='inner')
        
        if len(merged) == 0:
            return {'error': 'ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤'}
        
        # ì •ê·œí™” (0-1 ìŠ¤ì¼€ì¼)
        merged['ìœ„í—˜ì§€ìˆ˜_ì •ê·œí™”'] = (merged['ìœ„í—˜ì§€ìˆ˜'] - merged['ìœ„í—˜ì§€ìˆ˜'].min()) / \
                                  (merged['ìœ„í—˜ì§€ìˆ˜'].max() - merged['ìœ„í—˜ì§€ìˆ˜'].min())
        merged['ë³´ìƒê¸ˆ_ì •ê·œí™”'] = (merged['ë³´ìƒê¸ˆ'] - merged['ë³´ìƒê¸ˆ'].min()) / \
                                (merged['ë³´ìƒê¸ˆ'].max() - merged['ë³´ìƒê¸ˆ'].min())
        
        # ì˜ˆì¸¡ ì˜¤ì°¨ ê³„ì‚°
        merged['ì ˆëŒ€ì˜¤ì°¨'] = abs(merged['ìœ„í—˜ì§€ìˆ˜_ì •ê·œí™”'] - merged['ë³´ìƒê¸ˆ_ì •ê·œí™”'])
        merged['ì œê³±ì˜¤ì°¨'] = (merged['ìœ„í—˜ì§€ìˆ˜_ì •ê·œí™”'] - merged['ë³´ìƒê¸ˆ_ì •ê·œí™”']) ** 2
        
        # ì˜¤ë²„/ì–¸ë” ì˜ˆì¸¡ ë¶„ë¥˜
        merged['ì˜ˆì¸¡í¸í–¥'] = merged['ìœ„í—˜ì§€ìˆ˜_ì •ê·œí™”'] - merged['ë³´ìƒê¸ˆ_ì •ê·œí™”']
        merged['ì˜ˆì¸¡ìœ í˜•'] = merged['ì˜ˆì¸¡í¸í–¥'].apply(
            lambda x: 'ê³¼ëŒ€ì˜ˆì¸¡' if x > 0.1 else ('ê³¼ì†Œì˜ˆì¸¡' if x < -0.1 else 'ì ì •ì˜ˆì¸¡')
        )
        
        # ê·¸ë£¹ë³„ ì˜¤ë¥˜ ë¶„ì„
        if isinstance(group_by, str):
            group_by = [group_by]
        
        # êµ­ê°€ë³„ ì˜¤ë¥˜ íŒ¨í„´
        country_errors = merged.groupby('êµ­ê°€ëª…').agg({
            'ì ˆëŒ€ì˜¤ì°¨': ['mean', 'std'],
            'ì œê³±ì˜¤ì°¨': 'mean',
            'ì˜ˆì¸¡í¸í–¥': 'mean'
        }).round(4)
        
        # ì˜ˆì¸¡ ìœ í˜•ë³„ ë¶„í¬
        prediction_type_dist = merged['ì˜ˆì¸¡ìœ í˜•'].value_counts()
        
        # ìµœëŒ€ ì˜¤ì°¨ êµ­ê°€ë“¤
        top_error_countries = merged.nlargest(5, 'ì ˆëŒ€ì˜¤ì°¨')[['êµ­ê°€ëª…', 'ì ˆëŒ€ì˜¤ì°¨', 'ì˜ˆì¸¡ìœ í˜•']]
        
        # ì „ì²´ ì˜¤ë¥˜ í†µê³„
        overall_stats = {
            'í‰ê· ì ˆëŒ€ì˜¤ì°¨': round(merged['ì ˆëŒ€ì˜¤ì°¨'].mean(), 4),
            'í‰ê· ì œê³±ì˜¤ì°¨': round(merged['ì œê³±ì˜¤ì°¨'].mean(), 4),
            'í‰ê· í¸í–¥': round(merged['ì˜ˆì¸¡í¸í–¥'].mean(), 4),
            'ì˜ˆì¸¡ì •í™•ë„': round((merged['ì˜ˆì¸¡ìœ í˜•'] == 'ì ì •ì˜ˆì¸¡').mean(), 4)
        }
        
        error_analysis = {
            'ì „ì²´í†µê³„': overall_stats,
            'êµ­ê°€ë³„ì˜¤ë¥˜': country_errors.to_dict(),
            'ì˜ˆì¸¡ìœ í˜•ë¶„í¬': prediction_type_dist.to_dict(),
            'ìµœëŒ€ì˜¤ì°¨êµ­ê°€': top_error_countries.to_dict('records'),
            'ìƒì„¸ë°ì´í„°': merged
        }
        
    except Exception as e:
        error_analysis = {'error': str(e)}
    
    return error_analysis

def identify_blind_spots(error_analysis):
    """
    AI ëª¨ë¸ì˜ ì·¨ì•½ì (Blind Spots) ì‹ë³„
    
    error_analysis: ì˜¤ë¥˜ ë¶„ì„ ê²°ê³¼
    return: ê°œì„ ì´ í•„ìš”í•œ ì˜ì—­
    """
    blind_spots = {}
    
    try:
        if 'error' in error_analysis:
            return {'error': error_analysis['error']}
        
        merged_data = error_analysis['ìƒì„¸ë°ì´í„°']
        
        # 1. ê³ ìœ„í—˜ êµ­ê°€ ì¤‘ ê³¼ì†Œì˜ˆì¸¡ ì‚¬ë¡€
        high_loss_countries = merged_data[merged_data['ë³´ìƒê¸ˆ_ì •ê·œí™”'] > 0.7]
        under_predicted_high_risk = high_loss_countries[high_loss_countries['ì˜ˆì¸¡ìœ í˜•'] == 'ê³¼ì†Œì˜ˆì¸¡']
        
        # 2. ì €ìœ„í—˜ êµ­ê°€ ì¤‘ ê³¼ëŒ€ì˜ˆì¸¡ ì‚¬ë¡€  
        low_loss_countries = merged_data[merged_data['ë³´ìƒê¸ˆ_ì •ê·œí™”'] < 0.3]
        over_predicted_low_risk = low_loss_countries[low_loss_countries['ì˜ˆì¸¡ìœ í˜•'] == 'ê³¼ëŒ€ì˜ˆì¸¡']
        
        # 3. ì˜ˆì¸¡ ë³€ë™ì„±ì´ í° êµ­ê°€ë“¤
        if 'êµ­ê°€ë³„ì˜¤ë¥˜' in error_analysis:
            high_variance_countries = []
            for country, stats in error_analysis['êµ­ê°€ë³„ì˜¤ë¥˜'].items():
                if ('ì ˆëŒ€ì˜¤ì°¨', 'std') in stats and stats[('ì ˆëŒ€ì˜¤ì°¨', 'std')] > 0.2:
                    high_variance_countries.append(country)
        
        # 4. ê°œì„  ê¶Œê³ ì‚¬í•­
        recommendations = []
        
        if len(under_predicted_high_risk) > 0:
            recommendations.append({
                'ë¬¸ì œì˜ì—­': 'ê³ ì†ì‹¤ êµ­ê°€ ê³¼ì†Œì˜ˆì¸¡',
                'í•´ë‹¹êµ­ê°€': under_predicted_high_risk['êµ­ê°€ëª…'].tolist(),
                'ê¶Œê³ ì‚¬í•­': 'ê³ ìœ„í—˜ ì‹ í˜¸ ê°ì§€ ì•Œê³ ë¦¬ì¦˜ ë³´ê°• í•„ìš”'
            })
        
        if len(over_predicted_low_risk) > 0:
            recommendations.append({
                'ë¬¸ì œì˜ì—­': 'ì €ì†ì‹¤ êµ­ê°€ ê³¼ëŒ€ì˜ˆì¸¡',
                'í•´ë‹¹êµ­ê°€': over_predicted_low_risk['êµ­ê°€ëª…'].tolist(),
                'ê¶Œê³ ì‚¬í•­': 'ì•ˆì „ êµ­ê°€ íŒë³„ ê¸°ì¤€ ì •êµí™” í•„ìš”'
            })
        
        if len(high_variance_countries) > 0:
            recommendations.append({
                'ë¬¸ì œì˜ì—­': 'ì˜ˆì¸¡ ë³€ë™ì„± ê³¼ë‹¤',
                'í•´ë‹¹êµ­ê°€': high_variance_countries,
                'ê¶Œê³ ì‚¬í•­': 'ì˜ˆì¸¡ ì•ˆì •ì„± í–¥ìƒì„ ìœ„í•œ ëª¨ë¸ íŠœë‹ í•„ìš”'
            })
        
        # ì „ì²´ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ (ë³´í—˜ì—…ê³„ ë§ì¶¤í˜• ê°œì„ )
        accuracy = error_analysis['ì „ì²´í†µê³„']['ì˜ˆì¸¡ì •í™•ë„']
        
        # ë³´í—˜ì—…ê³„ ê´€ì ì—ì„œ ë³´ìˆ˜ì  ì˜ˆì¸¡ì˜ ê°€ì¹˜ ë°˜ì˜
        conservative_bias = error_analysis['ì „ì²´í†µê³„']['í‰ê· í¸í–¥']
        
        # ë³´ìˆ˜ì  ì˜ˆì¸¡ ë³´ë„ˆìŠ¤ (ê³¼ëŒ€ì˜ˆì¸¡ì´ ë§ì„ìˆ˜ë¡ ë³´í—˜ì—…ê³„ì— ìœ ë¦¬)
        conservative_bonus = min(conservative_bias * 0.5, 0.3) if conservative_bias > 0 else 0
        
        # ì¡°ì •ëœ ì„±ëŠ¥ ì ìˆ˜
        adjusted_performance = accuracy + conservative_bonus
        
        # ì™„í™”ëœ ê¸°ì¤€ ì ìš©
        if adjusted_performance > 0.4 or conservative_bias > 0.3:
            model_grade = 'ìš°ìˆ˜'
        elif adjusted_performance > 0.2 or conservative_bias > 0.2:
            model_grade = 'ë³´í†µ'
        else:
            model_grade = 'ê°œì„ í•„ìš”'
        
        blind_spots = {
            'ëª¨ë¸ì„±ëŠ¥ë“±ê¸‰': model_grade,
            'ì£¼ìš”ì·¨ì•½ì ': recommendations,
            'ê³¼ì†Œì˜ˆì¸¡_ê³ ìœ„í—˜êµ­ê°€': under_predicted_high_risk[['êµ­ê°€ëª…', 'ì ˆëŒ€ì˜¤ì°¨']].to_dict('records'),
            'ê³¼ëŒ€ì˜ˆì¸¡_ì €ìœ„í—˜êµ­ê°€': over_predicted_low_risk[['êµ­ê°€ëª…', 'ì ˆëŒ€ì˜¤ì°¨']].to_dict('records'),
            'ê°œì„ ìš°ì„ ìˆœìœ„': {
                '1ìˆœìœ„': 'ê³ ì†ì‹¤ êµ­ê°€ íƒì§€ ëŠ¥ë ¥ í–¥ìƒ',
                '2ìˆœìœ„': 'ì˜ˆì¸¡ ì•ˆì •ì„± ê°œì„ ', 
                '3ìˆœìœ„': 'ì €ìœ„í—˜ êµ­ê°€ ì •í™•ë„ í–¥ìƒ'
            }
        }
        
    except Exception as e:
        blind_spots = {'error': str(e)}
    
    return blind_spots

def calculate_risk_coverage(predicted, actual, threshold=0.8):
    """
    ìœ„í—˜ ì»¤ë²„ë¦¬ì§€ ê³„ì‚° (ìƒìœ„ X% ìœ„í—˜ êµ­ê°€ë¥¼ ì–¼ë§ˆë‚˜ ì •í™•íˆ ì˜ˆì¸¡í–ˆëŠ”ê°€)
    
    predicted: ì˜ˆì¸¡ ë°ì´í„°
    actual: ì‹¤ì œ ë°ì´í„°  
    threshold: ìƒìœ„ ë¹„ìœ¨ (0.8 = ìƒìœ„ 20%)
    return: ì»¤ë²„ë¦¬ì§€ ë¶„ì„ ê²°ê³¼
    """
    try:
        # êµ­ê°€ë³„ ì§‘ê³„
        pred_agg = predicted.groupby('êµ­ê°€ëª…')['ìœ„í—˜ì§€ìˆ˜'].mean().reset_index()
        actual_agg = actual.groupby('êµ­ê°€ëª…')['ë³´ìƒê¸ˆ'].sum().reset_index()
        
        merged = pd.merge(pred_agg, actual_agg, on='êµ­ê°€ëª…', how='inner')
        
        # ìƒìœ„ ìœ„í—˜ êµ­ê°€ ì‹ë³„ (ì‹¤ì œ)
        actual_threshold = merged['ë³´ìƒê¸ˆ'].quantile(threshold)
        actual_high_risk = set(merged[merged['ë³´ìƒê¸ˆ'] >= actual_threshold]['êµ­ê°€ëª…'])
        
        # ìƒìœ„ ìœ„í—˜ êµ­ê°€ ì‹ë³„ (ì˜ˆì¸¡)
        pred_threshold = merged['ìœ„í—˜ì§€ìˆ˜'].quantile(threshold)
        pred_high_risk = set(merged[merged['ìœ„í—˜ì§€ìˆ˜'] >= pred_threshold]['êµ­ê°€ëª…'])
        
        # ì»¤ë²„ë¦¬ì§€ ê³„ì‚°
        true_positives = len(actual_high_risk & pred_high_risk)
        false_negatives = len(actual_high_risk - pred_high_risk)
        false_positives = len(pred_high_risk - actual_high_risk)
        
        coverage = true_positives / len(actual_high_risk) if len(actual_high_risk) > 0 else 0
        precision = true_positives / len(pred_high_risk) if len(pred_high_risk) > 0 else 0
        
        return {
            'ì‹¤ì œ_ê³ ìœ„í—˜êµ­ê°€ìˆ˜': len(actual_high_risk),
            'ì˜ˆì¸¡_ê³ ìœ„í—˜êµ­ê°€ìˆ˜': len(pred_high_risk),
            'ì •í™•íˆ_ì˜ˆì¸¡í•œ_êµ­ê°€ìˆ˜': true_positives,
            'ì»¤ë²„ë¦¬ì§€': round(coverage, 4),
            'ì •ë°€ë„': round(precision, 4),
            'ëˆ„ë½ëœ_ê³ ìœ„í—˜êµ­ê°€': list(actual_high_risk - pred_high_risk),
            'ì˜ëª»_ì˜ˆì¸¡ëœ_êµ­ê°€': list(pred_high_risk - actual_high_risk)
        }
        
    except Exception as e:
        return {'error': str(e)} 