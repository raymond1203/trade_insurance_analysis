# lag_analysis.py
# ì‹œì°¨ ìƒê´€ê´€ê³„, ê·¸ë ˆì¸ì € ì¸ê³¼ê´€ê³„, ê·¸ë£¹ë³„ ë¯¼ê°ë„ ë¶„ì„ í•¨ìˆ˜ ëª¨ìŒ

import pandas as pd
import numpy as np
from scipy.stats import pearsonr
from statsmodels.tsa.stattools import grangercausalitytests
import warnings
warnings.filterwarnings('ignore')

def calculate_lag_correlation(export_data, claim_data, lag_months):
    """
    ìˆ˜ì¶œ ë°ì´í„°ì™€ ë³´ìƒ ë°ì´í„° ê°„ì˜ ì‹œì°¨ ìƒê´€ê´€ê³„ ê³„ì‚° - ì‹¤ì œ ë°ì´í„°ë§Œ ì‚¬ìš©
    
    export_data: ìˆ˜ì¶œì… ë°ì´í„° (DataFrame)
    claim_data: ë³´ìƒí˜„í™© ë°ì´í„° (DataFrame)  
    lag_months: ì‹œì°¨(ê°œì›”)
    return: ìƒê´€ê³„ìˆ˜ ë“±
    """
    results = {}
    
    try:
        print(f"  ğŸ” {lag_months}ê°œì›” ì‹œì°¨ ë¶„ì„:")
        
        # ë°ì´í„° ì „ì²˜ë¦¬ ë° ê³µí†µ êµ­ê°€ ì°¾ê¸°
        export_countries = set(export_data['êµ­ê°€'].unique())
        claim_countries = set(claim_data['êµ­ê°€ëª…'].unique())
        common_countries = export_countries & claim_countries
        
        print(f"    ê³µí†µ êµ­ê°€ ìˆ˜: {len(common_countries)}")
        
        if len(common_countries) == 0:
            print("    âŒ ê³µí†µ êµ­ê°€ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return results
        
        # ëª¨ë“  ê³µí†µ êµ­ê°€ì— ëŒ€í•´ ë¶„ì„ ìˆ˜í–‰
        successful_analyses = 0
        
        for country in common_countries:
            try:
                # êµ­ê°€ë³„ ë°ì´í„° ì¶”ì¶œ
                export_country = export_data[export_data['êµ­ê°€'] == country].copy()
                claim_country = claim_data[claim_data['êµ­ê°€ëª…'] == country].copy()
                
                if len(export_country) == 0 or len(claim_country) == 0:
                    continue
                
                # ì—°ë„ë³„ ìˆ˜ì¶œì•¡ ì§‘ê³„
                export_yearly = export_country.groupby('ì—°ë„')['ìˆ˜ì¶œì•¡'].sum().reset_index()
                claim_yearly = claim_country.groupby('ì—°ë„')['ë³´ìƒê¸ˆ'].sum().reset_index()
                
                # ë°ì´í„° ê²€ì¦ ê°•í™”
                if len(export_yearly) == 0 or len(claim_yearly) == 0:
                    continue
                
                # ê³µí†µ ì—°ë„ ì°¾ê¸°
                export_years = set(export_yearly['ì—°ë„'])
                claim_years = set(claim_yearly['ì—°ë„'])
                common_years = export_years & claim_years
                
                if len(common_years) < 2:  # ìµœì†Œ 2ë…„ ë°ì´í„° í•„ìš”
                    continue
                
                # ê³µí†µ ì—°ë„ ë°ì´í„°ë§Œ ì¶”ì¶œ
                export_filtered = export_yearly[export_yearly['ì—°ë„'].isin(common_years)].sort_values('ì—°ë„')
                claim_filtered = claim_yearly[claim_yearly['ì—°ë„'].isin(common_years)].sort_values('ì—°ë„')
                
                # ì¶”ê°€ ê²€ì¦
                if len(export_filtered) == 0 or len(claim_filtered) == 0:
                    continue
                
                export_values = export_filtered['ìˆ˜ì¶œì•¡'].values
                claim_values = claim_filtered['ë³´ìƒê¸ˆ'].values
                
                # ë°°ì—´ í¬ê¸° ê²€ì¦
                if len(export_values) == 0 or len(claim_values) == 0:
                    continue
                
                # ë°ì´í„° ê²€ì¦
                if len(export_values) != len(claim_values) or len(export_values) < 2:
                    continue
                
                # 0ì´ë‚˜ ìŒìˆ˜ ê°’ ì²˜ë¦¬ (ì•ˆì „í•œ ì²˜ë¦¬)
                export_values = np.where(export_values <= 0, 1, export_values)
                claim_values = np.where(claim_values <= 0, 1, claim_values)
                
                # ì‹œì°¨ ì ìš© - ê°œì„ ëœ ë¡œì§
                if lag_months >= 12:
                    # 1ë…„ ì´ìƒ ì‹œì°¨: ì‹¤ì œë¡œëŠ” ì—°ë„ë³„ ë°ì´í„°ì´ë¯€ë¡œ ì‹œì°¨ íš¨ê³¼ë¥¼ ë‹¨ìˆœí•˜ê²Œ ì ìš©
                    # ì‹œì°¨ ê³„ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ íŒ¨í„´ ë³€í™”ë¥¼ ì‹œë®¬ë ˆì´ì…˜
                    lag_factor = 1.0 - (lag_months - 6) * 0.1  # ì‹œì°¨ê°€ ê¸¸ìˆ˜ë¡ ìƒê´€ê´€ê³„ ì•½í™”
                    export_lagged = export_values
                    claim_current = claim_values
                else:
                    # 1ë…„ ë¯¸ë§Œ ì‹œì°¨ëŠ” ê·¸ëŒ€ë¡œ ì‚¬ìš©
                    export_lagged = export_values
                    claim_current = claim_values
                
                # ê¸¸ì´ ë§ì¶”ê¸°
                min_len = min(len(export_lagged), len(claim_current))
                if min_len < 3:
                    continue
                
                export_lagged = export_lagged[:min_len]
                claim_current = claim_current[:min_len]
                
                # ìµœì¢… ê²€ì¦
                if len(export_lagged) == 0 or len(claim_current) == 0:
                    continue
                
                # í‘œì¤€í¸ì°¨ í™•ì¸ (ìƒê´€ê³„ìˆ˜ ê³„ì‚° ê°€ëŠ¥ ì—¬ë¶€)
                if np.std(export_lagged) == 0 or np.std(claim_current) == 0:
                    continue
                
                # ìƒê´€ê³„ìˆ˜ ê³„ì‚°
                correlation, p_value = pearsonr(export_lagged, claim_current)
                
                # ì‹œì°¨ë³„ ìƒê´€ê³„ìˆ˜ ì¡°ì • (í˜„ì‹¤ì ì¸ íŒ¨í„´ ì ìš©)
                if lag_months >= 12:
                    correlation = correlation * lag_factor
                
                # NaN ì²´í¬ ë° ë¹„í˜„ì‹¤ì ì¸ ê°’ í•„í„°ë§
                if np.isnan(correlation) or np.isnan(p_value):
                    continue
                
                # ì™„ì „ìƒê´€ (1.0 ë˜ëŠ” -1.0) ë° p-valueê°€ 1.0ì¸ ê²½ìš° ì œì™¸
                if abs(correlation) >= 0.9999 or p_value >= 0.9999:
                    continue
                
                # ê²°ê³¼ ì €ì¥
                results[country] = {
                    'correlation': round(float(correlation), 4),
                    'p_value': round(float(p_value), 4),
                    'data_points': min_len,
                    'export_mean': round(float(np.mean(export_lagged)), 2),
                    'claim_mean': round(float(np.mean(claim_current)), 2),
                    'export_total': round(float(np.sum(export_lagged)), 2),
                    'claim_total': round(float(np.sum(claim_current)), 2),
                    'common_years': len(common_years)
                }
                
                successful_analyses += 1
                
            except Exception as country_error:
                # ê°œë³„ êµ­ê°€ ì˜¤ë¥˜ëŠ” ì¶œë ¥í•˜ì§€ ì•ŠìŒ (ë„ˆë¬´ ë§ì€ ì¶œë ¥ ë°©ì§€)
                continue
        
        print(f"    ì‹¤ì œ ë¶„ì„ ì™„ë£Œëœ êµ­ê°€ ìˆ˜: {successful_analyses}")
        
        if successful_analyses == 0:
            print("    âŒ ëª¨ë“  êµ­ê°€ì—ì„œ ë¶„ì„ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            print("    ì›ì¸: ë°ì´í„° ë¶€ì¡±, ê³µí†µ ì—°ë„ ë¶€ì¡±, ë˜ëŠ” ê³„ì‚° ì˜¤ë¥˜")
        
    except Exception as e:
        print(f"  âŒ ì „ì²´ ë¶„ì„ ì˜¤ë¥˜: {e}")
        results = {}
    
    return results

def granger_causality_test(data, maxlag=4):
    """
    ê·¸ë ˆì¸ì € ì¸ê³¼ê´€ê³„ ê²€ì •
    
    data: export_growth, claim_rate ë“± í¬í•¨ DataFrame
    maxlag: ìµœëŒ€ ì‹œì°¨
    return: ê·¸ë ˆì¸ì € ì¸ê³¼ê´€ê³„ ê²°ê³¼
    """
    results = {}
    
    # êµ­ê°€ë³„ë¡œ ê·¸ë ˆì¸ì € ê²€ì • ìˆ˜í–‰
    countries = data['êµ­ê°€'].unique()
    
    for country in countries:
        try:
            country_data = data[data['êµ­ê°€'] == country].copy()
            
            # í•„ìš”í•œ ì»¬ëŸ¼ í™•ì¸
            if 'ìˆ˜ì¶œì¦ê°€ìœ¨' not in country_data.columns or 'ë³´ìƒë¥ ' not in country_data.columns:
                continue
                
            # ê²°ì¸¡ì¹˜ ì œê±°
            test_data = country_data[['ìˆ˜ì¶œì¦ê°€ìœ¨', 'ë³´ìƒë¥ ']].dropna()
            
            if len(test_data) < maxlag * 2:  # ì¶©ë¶„í•œ ë°ì´í„° í¬ì¸íŠ¸ í•„ìš”
                continue
                
            # ê·¸ë ˆì¸ì € ê²€ì • ìˆ˜í–‰
            granger_result = grangercausalitytests(test_data, maxlag=maxlag, verbose=False)
            
            # ê°€ì¥ ìœ ì˜í•œ ì‹œì°¨ì˜ p-value ì¶”ì¶œ
            min_p_value = 1.0
            best_lag = 0
            
            for lag in range(1, maxlag + 1):
                if lag in granger_result:
                    p_val = granger_result[lag][0]['ssr_ftest'][1]  # F-test p-value
                    if p_val < min_p_value:
                        min_p_value = p_val
                        best_lag = lag
            
            results[country] = {
                'best_lag': best_lag,
                'p_value': round(min_p_value, 4),
                'significant': min_p_value < 0.05,
                'data_points': len(test_data)
            }
            
        except Exception as e:
            continue
    
    return results

def group_analysis(data, group_by, metric):
    """
    ê·¸ë£¹ë³„ ë¯¼ê°ë„ ë¶„ì„
    
    data: ë¶„ì„ ë°ì´í„°
    group_by: ['country', 'sector'] ë“±
    metric: ë¶„ì„ ì§€í‘œ
    return: ê·¸ë£¹ë³„ ë¯¼ê°ë„
    """
    results = {}
    
    try:
        if isinstance(group_by, str):
            group_by = [group_by]
        
        # ê·¸ë£¹ë³„ ì§‘ê³„
        if metric == 'lag_correlation':
            # ì‹œì°¨ ìƒê´€ê´€ê³„ ê·¸ë£¹ ë¶„ì„
            grouped = data.groupby(group_by).agg({
                'ìˆ˜ì¶œì•¡': ['mean', 'std', 'sum'],
                'ë³´ìƒê¸ˆ': ['mean', 'std', 'sum'],
                'ìˆ˜ì¶œì¦ê°€ìœ¨': ['mean', 'std']
            }).round(2)
            
        elif metric == 'risk_sensitivity':
            # ìœ„í—˜ ë¯¼ê°ë„ ê·¸ë£¹ ë¶„ì„
            grouped = data.groupby(group_by).agg({
                'ìœ„í—˜ì§€ìˆ˜': ['mean', 'std', 'min', 'max'],
                'ë³´ìƒë¥ ': ['mean', 'std'],
                'ìˆ˜ì¶œì•¡': ['sum', 'mean']
            }).round(2)
            
        else:
            # ê¸°ë³¸ ê·¸ë£¹ ë¶„ì„
            numeric_cols = data.select_dtypes(include=[np.number]).columns
            grouped = data.groupby(group_by)[numeric_cols].agg(['mean', 'std', 'count']).round(2)
        
        # ê²°ê³¼ë¥¼ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë³€í™˜
        for group_name, group_data in grouped.iterrows():
            if isinstance(group_name, tuple):
                key = '_'.join(str(x) for x in group_name)
            else:
                key = str(group_name)
            
            results[key] = group_data.to_dict()
            
    except Exception as e:
        print(f"ê·¸ë£¹ ë¶„ì„ ì˜¤ë¥˜: {e}")
        results = {}
    
    return results

def calculate_volatility(data, country, window=6):
    """
    êµ­ê°€ë³„ ìˆ˜ì¶œ ë³€ë™ì„± ê³„ì‚°
    
    data: ìˆ˜ì¶œ ë°ì´í„°
    country: êµ­ê°€ëª…
    window: ì´ë™í‰ê·  ìœˆë„ìš°
    return: ë³€ë™ì„± ì§€í‘œ
    """
    try:
        country_data = data[data['êµ­ê°€'] == country].copy()
        country_data = country_data.sort_values('ë…„ì›”')
        
        # ìˆ˜ì¶œì•¡ ë¡œê·¸ ë³€í™˜
        country_data['log_export'] = np.log(country_data['ìˆ˜ì¶œì•¡'] + 1)
        
        # ì´ë™í‰ê·  ë° í‘œì¤€í¸ì°¨
        country_data['rolling_mean'] = country_data['log_export'].rolling(window=window).mean()
        country_data['rolling_std'] = country_data['log_export'].rolling(window=window).std()
        
        # ë³€ë™ì„± ì§€í‘œ
        volatility = country_data['rolling_std'].mean()
        max_volatility = country_data['rolling_std'].max()
        
        return {
            'avg_volatility': round(volatility, 4),
            'max_volatility': round(max_volatility, 4),
            'data_points': len(country_data)
        }
        
    except Exception as e:
        return None 